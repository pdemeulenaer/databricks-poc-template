# JOB API V2.1
# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "11.3.x-cpu-ml-scala2.12"
    num_workers: 1
    # node_type_id: "Standard_E8_v3"
    # init_scripts:
    #   - dbfs:
    #       destination: "dbfs:/databricks/scripts/external-metastore.sh"    

  dev-pool-cluster: &dev-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "1118-212954-vole2-pool-eh309xxn" # "1027-141550-dogs36-pool-qq92uldu"

  staging-pool-cluster: &staging-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "1118-212646-ease1-pool-m497c557" # "1107-090611-akin92-pool-vroqllvm"

  prod-pool-cluster: &prod-pool-cluster
    new_cluster:
      <<: *basic-cluster-props
      instance_pool_id: "1118-212901-toner1-pool-rjt1zhfv" # "1107-090742-pews93-pool-71i9rjjm"            


environments:
  default:
    workflows:

      #############################################################
      # ETL workflow
      #############################################################
      - name: "template-etl-workflow"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster
        schedule:
          quartz_cron_expression: 0 0 0/2 * * ? # every 2 hours
          timezone_id: Europe/Kiev
          pause_status: UNPAUSED              
        tasks:
          - task_key: "step-etl-1-task"
            description: "Step etl-1: data generation"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/etl_1_data_generation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/etl_1_data_generation.yml" ]      
          - task_key: "step-etl-2-task"
            depends_on:
              - task_key: "step-etl-1-task"          
            description: "Step etl-2: features generation"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/etl_2_feature_generation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/etl_2_feature_generation.yml" ]                 

      #############################################################
      # Training workflow
      #############################################################
      # DEV
      - name: "template-train-workflow-dev"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster          
        tasks:
          - task_key: "step-training-task"
            description: "Step Training"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_training.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_training_dev.yml" ]
          - task_key: "step-validation-task"
            depends_on:
              - task_key: "step-training-task"
            job_cluster_key: "default"
            max_retries: 0            
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_validation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_validation_dev.yml" ]

      # STAGING
      - name: "template-train-workflow-staging"
        job_clusters:
          - job_cluster_key: "default"
            <<: *staging-pool-cluster   
        schedule:
          quartz_cron_expression: 0 0 0 * * ? # every day at midnight
          timezone_id: Europe/Kiev
          pause_status: UNPAUSED                      
        tasks:
          - task_key: "step-training-task"
            description: "Step Training"
            job_cluster_key: "default"
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_training.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_training_staging.yml" ]
          - task_key: "step-validation-task"
            depends_on:
              - task_key: "step-training-task"
            job_cluster_key: "default"
            max_retries: 0            
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_validation.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_validation_staging.yml" ]              

      #############################################################
      # Inference workflow
      #############################################################
      - name: "template-inference-workflow-dev"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster
        tasks:
          - task_key: "step-infer-task"
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_inference_dev.yml" ]         

      - name: "template-inference-workflow-uat"
        job_clusters:
          - job_cluster_key: "default"
            <<: *staging-pool-cluster
        # TODO: SCHEDULING SHOULD DEPEND ON ETL 1 AND 2 !!!
        tasks:
          - task_key: "step-infer-task"
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_inference_uat.yml" ]            

      - name: "template-inference-workflow-prod"
        job_clusters:
          - job_cluster_key: "default"
            <<: *prod-pool-cluster
        # TODO: SCHEDULING SHOULD DEPEND ON ETL 1 AND 2 !!!            
        tasks:
          - task_key: "step-infer-task"       
            description: "Step Inference Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_inference.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_inference_prod.yml" ]   

      #############################################################
      # Transition to Prod workflow
      #############################################################
      - name: "template-transition-to-prod-workflow"
        job_clusters:
          - job_cluster_key: "default"
            <<: *prod-pool-cluster          
        tasks:
          - task_key: step-transition-to-prod
            description: step transition to prod task
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_transition_to_prod.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_transition_to_prod.yml" ]                   

      #############################################################
      # Monitoring workflow
      #############################################################
      - name: "template-monitoring-workflow-dev"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-pool-cluster
        # TODO: SCHEDULING SHOULD DEPEND ON ETL 1 AND 2 !!!            
        tasks:
          - task_key: "step-monitoring-task"
            description: "Step monitoring Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_monitoring.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_monitoring_dev.yml" ]    

      - name: "template-monitoring-workflow-uat"
        job_clusters:
          - job_cluster_key: "default"
            <<: *staging-pool-cluster
        # TODO: SCHEDULING SHOULD DEPEND ON ETL 1 AND 2 !!!            
        tasks:
          - task_key: "step-monitoring-task"
            description: "Step monitoring Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_monitoring.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_monitoring_uat.yml" ]   

      - name: "template-monitoring-workflow-prod"
        job_clusters:
          - job_cluster_key: "default"
            <<: *prod-pool-cluster
        # TODO: SCHEDULING SHOULD DEPEND ON ETL 1 AND 2 !!!            
        tasks:
          - task_key: "step-monitoring-task"
            description: "Step monitoring Task"
            job_cluster_key: "default"
            libraries:
            - pypi:
                package: evidently
            max_retries: 0
            spark_python_task:
              python_file: "file://databricks_poc_template/tasks/task_monitoring.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/task_monitoring_prod.yml" ]                                                                 